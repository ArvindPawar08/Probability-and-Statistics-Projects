---
title: "Probability & Statistics Project 3"
author: "Arvind Pawar"
date: "November 21, 2018"
output: pdf_document
---


```{r echo=FALSE}
library(knitr)
library(kableExtra)
```
Probability Distribution is a function which is used to either generate or describe all likelihoods and possible values, which is taken by random variables in a given range. By understanding the dataset, we can plot the data and then plot various distribution and see which one most closely fits.    
   
Central Limit Theorem is an important concept in Inferential Statistics which shows us how the means of the samples of the same size taken from the same population vary about the population mean/   
   
We have a given a scenario of ancient Chinese game. In one electronic version of this game, a player selects 20 numbers from the set of numbers 1 through 100. The computer then randomly draws another set of 20 numbers from the set 1 through 100, and the player is rewarded according to how many of his selected numbers have been matched by the 20 numbers drawn by the computer.
Let X be the number of matches between a player's 20 selected numbers and the 20
numbers drawn by the computer. Then X may range from 0 (no match) to 20 (all match)
and follows a hyper-geometric probability distribution.  I have done this project in R programming.
   
__Part 1__
   
   a.	Construct Tabular Distribution for X. 
Introduction: To start the analysis of this data set, I have to calculate the probability for the number randomly picked by a user to be matched with a number randomly picked by computer and is calculated by the concept Hypergeometry Probability distribution function. Hypergeometry distribution is a discrete probability distribution that describes the probability of k successes which are random for which the object has drawn and from a finite population of size N that contains exactly K objects with that feature, and has a specified feature in n draws without replacement, wherein each draw is either a success or a failure.   
 __dhyper(c(0:20), m=20, n=80, k=20, log = FALSE)__  
      In the above code I have used dhype(x, m, n, k, log = FALSE) function which will generate hypergeometry distribution.
      
   x = Range of number of matches of the numbers picked by player and computer
      
   m = Numbers picked by player from the numbers 1 to 100
   
   n = Remaining numbers
   
   k = Numbers picked by computer
   
__Analysis:__
       
The player picks 20 numbers randomly, and the computer picks the 20 numbers randomly, and this sampling has done without replacement. So, we have used Hypergeometry Distribution. In Hypergeometry distribution the probability of success changes on each draw, as each draw decreases the population because the sampling was without replacement. In the given scenario we can see that there is more possibility that player will score 4.

   
__b.__   
Construct cumulative distribution for x   

By using __phyper(c(0:20), m=20, n=80, k=20, log = FALSE)__ I generated cumulative probability distribution.





```{r echo = FALSE}
in_x<-c(0:20)

hg <- dhyper(in_x,m=20,n=80,k=20,log = FALSE)

chg <- phyper(in_x,m=20,n=80,k=20,log = FALSE)

p1_table <- data.frame( c(0:20),hg,chg)
names(p1_table)<- c('x- Number of Matches','P(X = x)','	P(X <= x)')

kable(p1_table) %>% 
  kable_styling(bootstrap_options = "striped", full_width = F, position = "center")
```
__c.__   
__Introduction:__   
I have graphed Probability Distribution based on the values generated by Hypergeometry distribution. by using plot(dhyper(c(0:20), m=20, n=80, k=20, log = FALSE)), graphed the Hypergeometry Probability Distribution. 
__Analysis:__   
From the Hypergeometry Probability Distribution we can see that probability of getting score 8 or more is almost 0. There is more probability that 4 numbers will get matched with the numbers drawn by computer. The distribution right skewed.   

```{r echo=FALSE}

plot(hg,type='l')
title(main = "Hypergeometry Probability Distribution")
```
   
   
__d.__   
__Introduction:__   

I have graphed Cumulative Frequency graph based on the cumulative frequency distribution. Used plot(phyper(c(0:20), m=20, n=80, k=20, log = FALSE)) function to create graph.    

```{r echo=FALSE}

plot(chg,type = 'l')
title(main='Cummulative Distribution  keno ')
```   
__e.__   
__Introduction:__   
In this step, I have asked to calculate the theoretical mean, variance and standard deviation. 
* So, to calculate the theoretical mean I have to calculate the sum of the product of the number of matches and its corresponding probability distribution, so I used __sum(hg*c(0:20))__ function. 'hg' is variable where I stored Hypergeometry Distribution and c(0:20) is a count of matches from 0 to 20. The Theoretical mean is 4.   
* To calculate the variance the formula is    
__$variance=$$\sum$$(x-$$\mu$$)^2*P(X=x)$__
where x is the number of matches, $$\mu$$ is theoretical mean and P(X=x) is probability distribution.   
* To calculate variance, I executed below snippet,   
__hg.var<- sum((c(0:20)-hg.mean)^2*hg)__   
Assigned a variable hg.var and stored the value of variance in that variable.   
* Then, I calculated standard deviation which is square root of variance.    
__hg.sd <- sqrt(hg.var)__   
__Theoretical Mean__   

```{r echo=FALSE}
hg.mean <- sum(hg*c(0:20))
hg.mean
```   
__Theoretical Variance__   

```{r echo=FALSE}
hg.var <- sum(hg*c(0:20)*c(0:20))-(hg.mean)^2
hg.var
```   

__Theoretical Standard Deviation__   
```{r echo=FALSE}
hg.sd <- sqrt(hg.var)
hg.sd
```   
__Analysis:__   
The calculated values are theoretical values which are being calculated using formulas. From mean we can interpret that there are 4 numbers which can get matched with the numbers picked by player and the same by computer. After getting the mean I calculated the variance which tells us the spread between numbers in a data set.   

__f.__   
__Introduction:__
In this step I am going to Generate 1000 random values according to standard uniform probability distribution. I used runif function to generate 1000 uniformly distributed random values.   
__rv <- runif(1000)__   
__Analysis:__   
Many elements of statistical practice depend on randomness via random numbers. By generating this randomness, we can simulate the number of experiments which helps to predict what can be the results in some situations based on that we can take decisions. Which is creating a scenario of assuming the possibilities of getting the scores in Keno Game.    
__Conclusion:__   
Here I have generated 1000 random values based on standard uniform probability distribution which are similar to the probability distribution.    
__rv <- runif(1000)__
```{r echo=FALSE}
set.seed(1099)
rv <- runif(1000)
a <- vector(mode='double', length=1000L)
k <- 1

for (x in rv){
  a[k]<-min(which(chg>=x))-1
  k<-k+1
}
#Random<-read.csv("C:/Users/Arvind/Documents/c.csv")
#kable(Random)
```     
__g.__   
__Introduction:__   
I am going to simulate the probability distribution with the 1000 uniformly distributed random numbers.
I wrote the following code,   
__a <- vector(mode='double', length=1000L)__   

__k <- 1__   

__for (x in rv){__   

__a[k]<-min(which(chg>=x))-1__

__v<-v+1__

__}__   

First, I created a vector of type double because the values are in fractions. v is a counting variable. rv is variable where I stored uniformly distributed random values. And then I used for loop.    
__Analysis:__  
It has generated the 1000 simulations with respect to Cumulative Distribution and 1000 uniformly distributed random values. I observed that number that are occurring more number of times has high probability.   
__Conclusion:__   
This step has generated the values according to their probability distribution.   
__h.__   
__Introduction:__  
As we have generated a pattern based on the calculated probabilities is an experiment. Now, let's calculate the mean, variance and Standard Deviation of these experiments. 'a' is the vector where I have stored the experimental values and I used __mean(a), var(a), sd(a)__ to find the experimental mean, variance and Standard Deviation.   
__Analysis:__    
The values of theoretical and Experimental mean, variance and standard deviation are almost similar.   
```{r echo=FALSE}
ex.mean<-mean(a)
```   

```{r echo=FALSE}
ex.var<-var(a)
```   

```{r echo=FALSE}
ex.sd<-sd(a)

```   
 


```{r echo=FALSE}
Sub<-c('Expected Value of X', 'Variance of X', 'SD of X')
Theoretical<-c(hg.mean, hg.var, hg.sd)
Experimental<-c(ex.mean, ex.var, ex.sd)
p2_table<-data.frame(Sub, Theoretical, Experimental)
names(p2_table)<- c('', 'x- Number of Matches', 'Experimental (Simulated)')
kable(p2_table) %>% 
  kable_styling(bootstrap_options = "striped", full_width = F, position = "center", font_size = 15)

```   
__i.__   
Now let's calculate the mean successively from 20, 40, 60, 80, 100, 200, 300, 400, 500, 600, 700, 800, 900, and 1000 interval.    
I wrote below code to calculate mean.    
__sim.mean<-vector(mode='double', length=14)__   
__k<-1__   
__sim<- c(20, 40, 60, 80, 100, 200, 300, 400, 500, 600, 700, 800, 900,1000)__   
__for (x in sim){__   
__sim.mean[k]<-mean(a[1:x])__   
__k<- k+1__   
__}__   
It has calculated the mean of first 20 experimental values of x and then first 40 values and so on. I stored the values of means in vector 'sim.mean'.   
__The simulated means are:__   

```{r echo=FALSE}
sim.mean<-vector(mode='double', length=14)
k<-1
sim<- c(20, 40, 60, 80, 100, 200, 300, 400, 500, 600, 700, 800, 900,1000)
for (x in sim){
  sim.mean[k]<-mean(a[1:x])
  k<- k+1
}
```
```{r echo=FALSE}
th1.mean<-c(4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4)

```   

```{r echo=FALSE}
sim.mean
```   
__j.__   
__Introduction:__
After getting the mean values of experiment intervals, I am going to represent those values with the theoretical mean I have calculated earlier. To plot the graph, I used following code.   
__plot(sim, sim.mean, type="l", lwd=3, col="blue", xlim=c(0,1000), xaxs="i", yaxs="i")__   
__lines(sim,th1.mean, lwd=2, col="red")__   
__legend("topright", legend=c("y","z"), lwd=c(2,2), col=c("blue","red"))__   
__Analysis:__   
The red line represents the theoretical mean, and the blue line represents experimental interval means.  We can see the blue line starts from the top and then drops and comes near to that of theoretical mean. As the number of experiments increases the line of the experimental means overlaps the line of theoretical mean. Which is proving the 'The Law of Large Numbers' which states that if we do the experiments in large number, the experimental values become closer to theoretical values.



```{r echo=FALSE}
plot(sim, sim.mean, type="l", lwd=3, col="blue", xlim=c(0,1000), xaxs="i", yaxs="i")
lines(sim,th1.mean, lwd=2, col="red")
legend("topright", legend=c("Experimental Mean","Theoretical Mean"), lwd=c(2,2), col=c("blue","red"))
title(main='Line Plot of Experimetal mean values versus the number of simulation', cex.main = 1,   font.main= 4, col.main= "black", outer = FALSE)
```   
__Part 2__ 
```{r echo=FALSE}
set.seed(1099)
Edata<-read.csv("C:/Users/Arvind/Desktop/CPS 1st Quarter/Probability Theory & Statistics/Discussion Week 3/Project 3/Edata.csv",header=FALSE)
Edata<-unlist(Edata)
```
__a:__   
__Introduction:__   
I have given a normal population in worksheet. I have created the csv file of that data and read file in R to calculated Mean, Variance and Standard Deviation.   __Edata<-read.csv("C:/Users/Arvind/Desktop/Probability Theory & Statistics/Discussion Week 3/Project 3/Edata.csv",header=FALSE)__   
__Edata<-unlist(Edata)__   
__typeof(Edata)__  
__mean(Edata)__   
__var(Edata)__   
__sd(Edata)__   
To calculate mean, variance and standard deviation, I did above steps.    
    
The values are as follows.    
__Mean__
```{r echo=FALSE}
mean(Edata)
```   
__Variance__
```{r echo=FALSE}
var(Edata)
```   
__Standard Deviation__
```{r echo=FALSE}
sd(Edata)
```   
__Analysis:__   
From Standard deviation 40.12618 we can conclude that we can cover almost 80% population if we go one standard deviation away from mean to left or right.       
__b.__   
__Introduction:__   
Now I have to construct a relative frequency for the given population. To construct the relative frequency Histogram I used hist(Edata, freq=FALSE), This function in R, automatically calculates relative frequency and plot the histogram.   
__Analysis:__   
From the shape of the histogram we can say that it is not random, most of our population set is between range 0-20 and 5% of the population is between 90-100.

```{r echo=FALSE}
hist(Edata, freq=FALSE,main=NULL)
title(main='Relative Frequency Distribution')
```      
__c:__   
__Introduction:__   
I have to generate 30 samples of 30 random numbers each from the Normal Population. For this I wrote below code.   
__list30<- vector(mode='list', length=30L)__   
__count<-1__   

__for(val in c(1:30))__   
__{__    
__list30[[count]]<- sample(Edata,30,replace=FALSE)__   
__count<-count+1__   
__}__   
I have created vector list30 which will store the 30 samples. Sample function will take 30 random numbers from normal population without repetition and have put it into for loop to generate 30 samples. 

```{r echo=FALSE ,include=FALSE}
list30<-vector(mode='list', length=30L)
count<-1
for(val in c(1:30))
{ 
  list30[[count]]<- sample(Edata,30,replace=FALSE)
  count<-count+1
}

```   
__d:__   
__Introduction:__   
Now I have to calculate the mean, variance and standard deviation for each sample. I have calculated it using __list30_mean <- unlist(lapply(list30, mean))__, have created variable list30_mean, where I have store the mean of the sample. I used lapply function which calculates the mean of each sample at a time. After executing this function, I got mean values of all 30 samples. I applied the unlist function to convert the values of samples into integers because the samples are in the list and we cannot make any calculations on a list. Similarly, I followed these steps to calculate variance and Standard deviation of all the 30 samples.    
__list30_var <- unlist(lapply(list30, var))__   
__list30_sd <- unlist(lapply(list30, sd))__    
__Analysis:__   
As per my observation I see all 30 samples have almost nearby values of means, variances and standard deviation are almost similar.  The mean of the samples represents the whole population from where we have taken samples.   
__e:__   
I am going to calculate average of means, variances and standard deviations.    
__final_mean <- mean(list30_mean)__   
__Var_mean <- mean(list30_var)__   
__sd_mean<- mean(list30_sd)__      
__f:__   

__Introduction:__   
I am going to comparing the mean, variance and standard deviation of samples and the same of the population from where we have taken samples.    
__Analysis:__   
All these values are almost like the population mean, variance and standard deviation., this is a proof of one of the points of Central Limit Theorem, that sample taken from the population will have a similar mean, variance and standard deviation to that of the whole population.    
__g:__   
Now I am going to create relative frequency distribution of means of 30 samples.    
__Analysis:__   
The shape of the relative frequency looks like a normal distribution, not exactly but alike, which is proving the other point of Central Limit Theorem which states that If samples taken from population are of a large enough size, then the distribution of their averages will approximately be a normal distribution.   


```{r echo=FALSE}
list30_mean <- unlist(lapply(list30, mean))
#list30_mean
final_mean <- mean(list30_mean)

list30_var <- unlist(lapply(list30, var))

Var_mean <- mean(list30_var)

list30_sd <- unlist(lapply(list30, sd))

sd_mean<- mean(list30_sd)

```      

```{r echo=FALSE}   
hist(list30_mean, freq=FALSE, main=NULL)
title(main='Relative frequncy histogram for the 30 sample means')
```   
__h:__   
From all the above steps and analysis, I can conclude that if we select a large number of samples from the population, the mean, variance and standard deviation will be similar, and the distribution tends to be normal. When we select a large number of samples, it decreases the risk of outcomes.   

I have done the module 3 project in R, and I got comfortable with a portion of the capacity in R programming. This undertaking gave me a chance to build up a logic while doing a few stages like producing 1000 uniformly distributed random numbers with cumulative distribution, creating 30 tests from a normal population. I additionally came to realize that how random numbers help to extend tests likewise, the tasks performed all in all samples and population comprehends the ideas of as far as a possible hypothesis. The qualities we ascertained hypothetical and the qualities we got from trials were relatively comparative which demonstrates 'The Law of Large Numbers', as we have done the experiments number of times utilizing uniformly distributed random numbers and cumulative distribution.   
   
      
__References:__   
1. (n.d.). Retrieved from https://www.cliffsnotes.com/study-guides/statistics/sampling/central-limit-theorem   
2. R Language what is difference between rnorm and runif. (n.d.). Retrieved from https://stats.stackexchange.com/questions/49370/r-language-what-is-difference-between-rnorm-and-runif   
3. (n.d.). Retrieved from https://web.ma.utexas.edu/users/mks/statmistakes/skeweddistributions.html   
4.Skewness. (2018, November 25). Retrieved from https://en.wikipedia.org/wiki/Skewness   
5. Stat Trek. (n.d.). Retrieved from https://stattrek.com/probability-distributions/hypergeometric.aspx    
6. Hypergeometric distribution. (2018, October 25). Retrieved from https://en.wikipedia.org/wiki/Hypergeometric_distribution#Multivariate_hypergeometric_distribution   
7. https://northeastern.acrobatiq.com/courseware/NEU_MPSA_PROB_THEORY_INTRO_STATS_ALY6010_V2_2/week_3_-_probability_distributions_and_the_central_limit_theorem__sampling/module_overview__probability_distributions_and_the_central_limit_theorem__sampling/wbp_module_overview__probability_distributions_and_the_central_limit_theorem__sampling




